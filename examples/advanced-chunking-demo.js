/**
 * Advanced Chunking Demo with Forced Large Transcript
 * 
 * This demo creates a truly large transcript that will trigger chunking
 * to demonstrate the advanced chunking strategy in action.
 */

const { ChunkingStrategy, CHUNKING_STRATEGIES } = require('../src/utils/chunkingStrategy.js');

// Create a massive transcript that will definitely need chunking
function createMassiveTranscript() {
    const speakers = ['AliceæŠ€è¡“é•·', 'Bobå°ˆæ¡ˆç¶“ç†', 'Charlieé–‹ç™¼', 'Dianaè¨­è¨ˆ', 'Eveæ¸¬è©¦', 'Frankè¡ŒéŠ·'];
    const sections = [];
    
    const topics = [
        'ç³»çµ±æ¶æ§‹è¨­è¨ˆè¨è«–',
        'å¾Œç«¯APIé–‹ç™¼é€²åº¦å ±å‘Š',
        'å‰ç«¯ä»‹é¢è¨­è¨ˆè©•ä¼°',
        'è³‡æ–™åº«æ•ˆèƒ½å„ªåŒ–',
        'ä½¿ç”¨è€…é«”é©—æ¸¬è©¦çµæœ',
        'å¸‚å ´æ¨å»£ç­–ç•¥è¦åŠƒ',
        'å®‰å…¨æ€§æª¢æŸ¥å ±å‘Š',
        'å°ˆæ¡ˆæ™‚ç¨‹èª¿æ•´è¨è«–',
        'è·¨éƒ¨é–€å”ä½œæµç¨‹',
        'æŠ€è¡“å‚µå‹™è™•ç†è¨ˆç•«'
    ];
    
    // Create 500 sections with substantial content
    for (let i = 0; i < 500; i++) {
        const speaker = speakers[i % speakers.length];
        const topic = topics[i % topics.length];
        const minutes = Math.floor(i * 0.5);
        const seconds = (i * 30) % 60;
        const startTime = `${Math.floor(minutes/60).toString().padStart(2, '0')}:${(minutes%60).toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
        const endTime = `${Math.floor((minutes+1)/60).toString().padStart(2, '0')}:${((minutes+1)%60).toString().padStart(2, '0')}:${((seconds+30)%60).toString().padStart(2, '0')}`;
        
        // Generate substantial content for each section
        const baseContent = `é—œæ–¼${topic}ï¼Œæˆ‘æƒ³åˆ†äº«ä»¥ä¸‹å¹¾å€‹é‡é»ï¼š`;
        const detailedContent = `
é¦–å…ˆï¼Œå¾æŠ€è¡“è§’åº¦ä¾†çœ‹ï¼Œæˆ‘å€‘éœ€è¦è€ƒæ…®çš„å› ç´ åŒ…æ‹¬ç³»çµ±çš„å¯æ“´å±•æ€§ã€ç¶­è­·æ€§å’Œå®‰å…¨æ€§ã€‚é€™äº›å› ç´ æœƒç›´æ¥å½±éŸ¿åˆ°æˆ‘å€‘çš„é–‹ç™¼æ•ˆç‡å’Œç”¢å“å“è³ªã€‚

å…¶æ¬¡ï¼Œæˆ‘å€‘å¿…é ˆè©•ä¼°ç•¶å‰çš„è³‡æºé…ç½®æ˜¯å¦è¶³å¤ æ”¯æŒé€™å€‹è¨ˆç•«ã€‚åŒ…æ‹¬äººåŠ›è³‡æºã€æŠ€è¡“è³‡æºã€æ™‚é–“è³‡æºç­‰å„å€‹æ–¹é¢çš„è€ƒé‡éƒ½éå¸¸é‡è¦ã€‚

ç¬¬ä¸‰ï¼Œæˆ‘å»ºè­°æˆ‘å€‘æ¡ç”¨æ•æ·é–‹ç™¼çš„æ–¹æ³•ï¼Œé€éçŸ­æœŸè¿­ä»£ä¾†å¿«é€Ÿé©—è­‰æˆ‘å€‘çš„å‡è¨­å’Œæ–¹æ¡ˆã€‚é€™æ¨£å¯ä»¥é™ä½é¢¨éšªï¼ŒåŒæ™‚æé«˜äº¤ä»˜å“è³ªã€‚

å¦å¤–ï¼Œè·¨éƒ¨é–€çš„æºé€šå”èª¿ä¹Ÿæ˜¯æˆåŠŸçš„é—œéµå› ç´ ã€‚æˆ‘å€‘éœ€è¦å»ºç«‹æœ‰æ•ˆçš„æºé€šæ©Ÿåˆ¶ï¼Œç¢ºä¿è³‡è¨Šé€æ˜ï¼Œé¿å…é‡è¤‡å·¥ä½œã€‚

æœ€å¾Œï¼Œé—œæ–¼æ™‚ç¨‹å®‰æ’ï¼Œæˆ‘èªç‚ºæˆ‘å€‘æ‡‰è©²è¦ä¿ç•™ä¸€å®šçš„ç·©è¡æ™‚é–“ï¼Œä»¥æ‡‰å°å¯èƒ½å‡ºç¾çš„æŠ€è¡“å›°é›£æˆ–éœ€æ±‚è®Šæ›´ã€‚åŒæ™‚ä¹Ÿè¦å®šæœŸæª¢è¦–é€²åº¦ï¼Œå¿…è¦æ™‚èª¿æ•´ç­–ç•¥ã€‚

ä»¥ä¸Šæ˜¯æˆ‘å°${topic}çš„åˆæ­¥æƒ³æ³•ï¼Œæ­¡è¿å¤§å®¶æå‡ºæ„è¦‹å’Œå»ºè­°ã€‚æˆ‘å€‘å¯ä»¥é€²ä¸€æ­¥è¨è«–å…·é«”çš„å¯¦æ–½ç´°ç¯€å’Œåˆ†å·¥å®‰æ’ã€‚
        `.repeat(3); // Repeat to make it really large
        
        sections.push({
            speaker,
            startTime,
            endTime,
            text: baseContent + detailedContent,
            confidence: 0.85 + Math.random() * 0.15
        });
    }
    
    const content = sections.map(section => 
        `[${section.startTime}] ${section.speaker}: ${section.text}`
    ).join('\n');
    
    return {
        metadata: {
            participants: speakers,
            duration: '04:10:00',
            language: 'zh-tw',
            totalEntries: sections.length,
            startTime: sections[0].startTime,
            endTime: sections[sections.length - 1].endTime
        },
        content,
        sections
    };
}

// Mock AI function that simulates processing
async function mockAIProcessor(transcript, options = {}) {
    const delay = 200 + Math.random() * 300; // Simulate processing time
    await new Promise(resolve => setTimeout(resolve, delay));
    
    const isChunk = options.isChunk;
    const isCombinung = options.isCombinung;
    const chunkIndex = options.chunkIndex || 0;
    
    if (isCombinung) {
        return {
            summary: `## ç¶œåˆæœƒè­°æ‘˜è¦\n\næœ¬æ¬¡æœƒè­°ç¸½å…±è™•ç†äº†${options.totalSections}å€‹è¨è«–æ®µè½ï¼Œæ¶µè“‹äº†æŠ€è¡“é–‹ç™¼ã€å°ˆæ¡ˆç®¡ç†ã€è¨­è¨ˆå„ªåŒ–ç­‰å¤šå€‹é¢å‘ã€‚\n\n## ä¸»è¦æˆæœ\n- ç¢ºç«‹äº†ç³»çµ±æ¶æ§‹æ–¹å‘\n- å„ªåŒ–äº†é–‹ç™¼æµç¨‹\n- å¼·åŒ–äº†è·¨éƒ¨é–€å”ä½œ\n\n## å¾ŒçºŒè¡Œå‹•\n- æŠ€è¡“å¯¦ä½œæ¨é€²\n- å“è³ªç®¡æ§åŠ å¼·\n- æ™‚ç¨‹ç›£æ§èª¿æ•´`,
            metadata: {
                generatedAt: new Date().toISOString(),
                model: options.model || 'gpt-4.1',
                processingTime: delay,
                usage: { total_tokens: 2000 + Math.floor(Math.random() * 500) }
            }
        };
    }
    
    if (isChunk) {
        return {
            summary: `## æœƒè­°æ®µè½ ${chunkIndex + 1} æ‘˜è¦\n\næ­¤æ®µè½ä¸»è¦è¨è«–äº†${transcript.metadata.participants.slice(0,3).join('ã€')}ç­‰åƒèˆ‡è€…æå‡ºçš„é‡è¦è­°é¡Œã€‚\n\n### è¨è«–é‡é»\n- æŠ€è¡“æ–¹æ¡ˆè©•ä¼°\n- é€²åº¦ç‹€æ³å›å ±\n- å•é¡Œè§£æ±ºæ–¹æ¡ˆ\n\n### æ±ºç­–äº‹é …\n- ç¢ºèªæŠ€è¡“æ–¹å‘\n- èª¿æ•´è³‡æºé…ç½®\n- å„ªåŒ–æµç¨‹æ–¹æ³•\n\n### å¾ŒçºŒè¿½è¹¤\n- å¯¦ä½œç´°ç¯€ç¢ºèª\n- æ™‚ç¨‹å®‰æ’èª¿æ•´\n- å“è³ªæ¨™æº–åˆ¶å®š`,
            metadata: {
                generatedAt: new Date().toISOString(),
                model: options.model || 'gpt-4.1',
                processingTime: delay,
                usage: { total_tokens: 800 + Math.floor(Math.random() * 200) },
                chunkInfo: {
                    chunkIndex,
                    totalChunks: options.totalChunks,
                    timeRange: `${transcript.metadata.startTime} - ${transcript.metadata.endTime}`
                }
            }
        };
    }
    
    // Regular summary
    return {
        summary: `## å®Œæ•´æœƒè­°æ‘˜è¦\n\né€™å ´é•·æ™‚é–“çš„æœƒè­°æ¶µè“‹äº†å¤šå€‹é‡è¦è­°é¡Œï¼Œåƒèˆ‡è€…åŒ…æ‹¬${transcript.metadata.participants.join('ã€')}ç­‰è³‡æ·±åŒäº‹ã€‚\n\n## ä¸»è¦è¨è«–å…§å®¹\n- ç³»çµ±æ¶æ§‹è¨­è¨ˆèˆ‡å„ªåŒ–\n- é–‹ç™¼é€²åº¦èˆ‡å“è³ªæ§ç®¡\n- è·¨éƒ¨é–€å”ä½œæ©Ÿåˆ¶\n- å¸‚å ´æ¨å»£ç­–ç•¥\n\n## é‡è¦æ±ºç­–\n- æ¡ç”¨æ–°çš„æŠ€è¡“æ¶æ§‹\n- èª¿æ•´å°ˆæ¡ˆæ™‚ç¨‹\n- å¼·åŒ–å“è³ªæª¢æŸ¥æµç¨‹\n\n## è¡Œå‹•è¨ˆç•«\n- æŠ€è¡“å¯¦ä½œåŠ é€Ÿæ¨é€²\n- å®šæœŸé€²åº¦æª¢è¦–\n- è·¨éƒ¨é–€æºé€šå¼·åŒ–`,
        metadata: {
            generatedAt: new Date().toISOString(),
            model: options.model || 'gpt-4.1',
            processingTime: delay,
            usage: { total_tokens: 1200 + Math.floor(Math.random() * 300) }
        }
    };
}

async function demonstrateLargeTranscriptChunking() {
    console.log('ğŸš€ Advanced Chunking Strategy - Large Transcript Demo\n');
    console.log('=' * 60);
    
    const strategy = new ChunkingStrategy();
    const massiveTranscript = createMassiveTranscript();
    
    console.log('\nğŸ“Š Massive Transcript Statistics:');
    console.log(`Duration: ${massiveTranscript.metadata.duration}`);
    console.log(`Participants: ${massiveTranscript.metadata.participants.length}`);
    console.log(`Sections: ${massiveTranscript.sections.length}`);
    console.log(`Content size: ${massiveTranscript.content.length.toLocaleString()} characters`);
    
    // Force chunking by using a very small token limit
    const smallTokenLimit = 15000; // Force chunking
    
    console.log('\nğŸ” Chunking Analysis:');
    
    const providers = [
        { name: 'OpenAI GPT-4.1', provider: 'openai', model: 'gpt-4.1' },
        { name: 'Claude Sonnet 4', provider: 'anthropic', model: 'claude-3-5-sonnet-20241022' }
    ];
    
    for (const config of providers) {
        console.log(`\n  ${config.name}:`);
        
        const analysis = strategy.analyzeChunkingNeeds(massiveTranscript, config.provider, config.model);
        console.log(`    Token estimate: ${analysis.tokenCount.toLocaleString()}`);
        console.log(`    Context limit: ${analysis.contextLimit.toLocaleString()}`);
        console.log(`    Needs chunking: ${analysis.needsChunking}`);
        console.log(`    Estimated chunks: ${analysis.estimatedChunks}`);
        console.log(`    Complexity: ${analysis.complexity}`);
        
        // Force chunking with small limit
        const chunks = strategy.chunkTranscript(massiveTranscript, {
            provider: config.provider,
            model: config.model,
            maxTokensPerChunk: smallTokenLimit,
            strategy: CHUNKING_STRATEGIES.HYBRID
        });
        
        console.log(`    Forced chunks (${smallTokenLimit.toLocaleString()} tokens limit): ${chunks.length}`);
        console.log(`    Avg chunk size: ${Math.round(chunks.reduce((sum, c) => sum + strategy.estimateTokenCount(c.content), 0) / chunks.length).toLocaleString()} tokens`);
        
        if (chunks.length > 0) {
            console.log(`    First chunk time: ${chunks[0].metadata.timeRange.start} - ${chunks[0].metadata.timeRange.end}`);
            console.log(`    Last chunk time: ${chunks[chunks.length-1].metadata.timeRange.start} - ${chunks[chunks.length-1].metadata.timeRange.end}`);
        }
    }
    
    // Demonstrate full processing with chunking
    console.log('\nğŸ”„ Full Processing with Chunking (OpenAI GPT-4.1):');
    console.log(`Forcing chunking with ${smallTokenLimit.toLocaleString()} token limit per chunk...\n`);
    
    let progressCount = 0;
    const progressCallback = (update) => {
        progressCount++;
        const timestamp = new Date().toLocaleTimeString();
        console.log(`[${timestamp}] ${update.stage.toUpperCase()}: ${update.message}`);
        
        if (update.chunkInfo) {
            console.log(`  â””â”€ Time: ${update.chunkInfo.timeRange}, Speakers: ${update.chunkInfo.speakers.length}, Tokens: ${update.chunkInfo.tokenCount.toLocaleString()}`);
        }
    };
    
    try {
        const startTime = Date.now();
        
        const result = await strategy.processLargeTranscript(
            massiveTranscript,
            mockAIProcessor,
            {
                provider: 'openai',
                model: 'gpt-4.1',
                strategy: CHUNKING_STRATEGIES.HYBRID,
                language: 'zh-TW',
                maxTokensPerChunk: smallTokenLimit
            },
            progressCallback
        );
        
        const endTime = Date.now();
        const totalTime = endTime - startTime;
        
        console.log('\nâœ… Processing Complete!');
        console.log(`Total processing time: ${totalTime}ms`);
        console.log(`Progress updates: ${progressCount}`);
        console.log(`Processing method: ${result.metadata.processingMethod}`);
        console.log(`Chunks processed: ${result.metadata.chunksProcessed}`);
        console.log(`Chunks failed: ${result.metadata.chunksFailed}`);
        
        if (result.metadata.chunkingSummary) {
            console.log(`Chunking strategy: ${result.metadata.chunkingSummary.strategy || 'Not specified'}`);
            console.log(`Total tokens processed: ${result.metadata.chunkingSummary.totalTokens?.toLocaleString() || 'Unknown'}`);
            console.log(`Average chunk size: ${result.metadata.chunkingSummary.avgChunkSize?.toLocaleString() || 'Unknown'} tokens`);
        }
        
        console.log('\nğŸ“„ Final Summary (first 500 chars):');
        console.log(result.summary.substring(0, 500) + '...');
        
        console.log('\nğŸ“Š Chunk Processing Details:');
        if (result.chunkDetails) {
            result.chunkDetails.forEach((detail, index) => {
                const status = detail.success ? 'âœ…' : 'âŒ';
                console.log(`  ${status} Chunk ${index + 1}: ${detail.timeRange.start}-${detail.timeRange.end} (${detail.tokenCount.toLocaleString()} tokens)`);
            });
        }
        
        console.log('\nğŸ¯ Performance Metrics:');
        console.log(`Average processing time per chunk: ${Math.round(totalTime / (result.metadata.chunksProcessed || 1))}ms`);
        console.log(`Tokens per second: ${Math.round((result.metadata.chunkingSummary?.totalTokens || 0) * 1000 / totalTime).toLocaleString()}`);
        
    } catch (error) {
        console.error('\nâŒ Processing failed:', error.message);
        console.error('Stack trace:', error.stack);
    }
    
    console.log('\nğŸ Advanced Chunking Demo Complete!');
    console.log('\nAdvanced Features Demonstrated:');
    console.log('âœ“ Large transcript handling (500 sections, 100k+ characters)');
    console.log('âœ“ Intelligent chunking with multiple strategies');
    console.log('âœ“ Context overlap for conversation continuity');
    console.log('âœ“ Real-time progress tracking');
    console.log('âœ“ Multi-provider optimization (OpenAI vs Claude)');
    console.log('âœ“ Error handling and recovery');
    console.log('âœ“ Performance monitoring and metrics');
    console.log('âœ“ Comprehensive metadata enrichment');
    console.log('âœ“ Chinese language support');
}

// Run the demonstration
if (require.main === module) {
    demonstrateLargeTranscriptChunking().catch(console.error);
}

module.exports = {
    createMassiveTranscript,
    mockAIProcessor,
    demonstrateLargeTranscriptChunking
};