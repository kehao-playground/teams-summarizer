/**
 * Chunking Strategy Demonstration
 * 
 * This demo shows how the advanced chunking strategy handles large transcripts
 * with different scenarios and providers.
 */

// Import the chunking strategy
const { ChunkingStrategy, CHUNKING_STRATEGIES, PROVIDER_LIMITS } = require('../src/utils/chunkingStrategy.js');

// Create demo transcript data
function createDemoTranscript(sizeCategory = 'large') {
    const baseSections = [
        {
            speaker: 'ç‹ç¶“ç†',
            startTime: '00:00:00',
            endTime: '00:03:15',
            text: 'å¤§å®¶æ—©å®‰ï¼Œæ­¡è¿åƒåŠ ä»Šå¤©çš„ç”¢å“è¦åŠƒæœƒè­°ã€‚ä»Šå¤©æˆ‘å€‘è¦è¨è«–Q2çš„ç”¢å“é–‹ç™¼ç­–ç•¥ï¼ŒåŒ…æ‹¬æ–°åŠŸèƒ½çš„å„ªå…ˆç´šæ’åºã€æŠ€è¡“æ¶æ§‹æ±ºç­–ï¼Œä»¥åŠå¸‚å ´æ¨å»£è¨ˆç•«ã€‚é¦–å…ˆè«‹å„éƒ¨é–€å ±å‘Šç›®å‰çš„é€²åº¦ç‹€æ³ã€‚',
            confidence: 0.95
        },
        {
            speaker: 'å¼µé–‹ç™¼',
            startTime: '00:03:16',
            endTime: '00:08:45',
            text: 'è¬è¬ç¶“ç†ã€‚é–‹ç™¼éƒ¨é€™é‚Šçš„é€²åº¦å ±å‘Šå¦‚ä¸‹ï¼šç”¨æˆ¶èªè­‰ç³»çµ±å·²ç¶“å®Œæˆä¸¦é€šéæ¸¬è©¦ï¼Œç›®å‰æ­£åœ¨é€²è¡ŒAPIå®‰å…¨æ€§å¼·åŒ–ã€‚è³‡æ–™åº«å„ªåŒ–çš„éƒ¨åˆ†ï¼Œæˆ‘å€‘æ¡ç”¨äº†æ–°çš„ç´¢å¼•ç­–ç•¥ï¼ŒæŸ¥è©¢æ•ˆèƒ½æå‡äº†ç´„40%ã€‚æ¥ä¸‹ä¾†æœƒå°ˆæ³¨åœ¨å‰ç«¯ä»‹é¢çš„é‡æ§‹å’Œç§»å‹•ç«¯é©é…ã€‚',
            confidence: 0.92
        },
        {
            speaker: 'æè¨­è¨ˆ',
            startTime: '00:08:46',
            endTime: '00:13:20',
            text: 'è¨­è¨ˆåœ˜éšŠé€™é‚Šå·²ç¶“å®Œæˆäº†æ–°ç‰ˆUIè¨­è¨ˆç¨¿ï¼Œä½¿ç”¨è€…é«”é©—æ¸¬è©¦çµæœå¾ˆæ­£é¢ã€‚æˆ‘å€‘é‡æ–°è¨­è¨ˆäº†å°èˆªçµæ§‹ï¼Œç°¡åŒ–äº†æ“ä½œæµç¨‹ï¼Œé è¨ˆå¯ä»¥æå‡30%çš„ç”¨æˆ¶ç•™å­˜ç‡ã€‚å¦å¤–ï¼Œæˆ‘å€‘ä¹Ÿå®Œæˆäº†æ·±è‰²æ¨¡å¼çš„è¨­è¨ˆï¼Œç¬¦åˆç¾ä»£ç”¨æˆ¶çš„ä½¿ç”¨ç¿’æ…£ã€‚',
            confidence: 0.89
        },
        {
            speaker: 'é™³æ¸¬è©¦',
            startTime: '00:13:21',
            endTime: '00:18:10',
            text: 'æ¸¬è©¦éƒ¨é–€çš„å ±å‘Šï¼šæˆ‘å€‘å·²ç¶“å»ºç«‹äº†è‡ªå‹•åŒ–æ¸¬è©¦æµç¨‹ï¼ŒåŒ…æ‹¬å–®å…ƒæ¸¬è©¦ã€æ•´åˆæ¸¬è©¦å’Œç«¯å°ç«¯æ¸¬è©¦ã€‚æ¸¬è©¦è¦†è“‹ç‡é”åˆ°85%ï¼Œç¬¦åˆå…¬å¸æ¨™æº–ã€‚ç›®å‰æ­£åœ¨é€²è¡Œæ€§èƒ½æ¸¬è©¦ï¼Œç™¼ç¾å¹¾å€‹ç“¶é ¸é»ï¼Œå·²ç¶“èˆ‡é–‹ç™¼åœ˜éšŠå”èª¿è§£æ±ºæ–¹æ¡ˆã€‚',
            confidence: 0.91
        },
        {
            speaker: 'é»ƒè¡ŒéŠ·',
            startTime: '00:18:11',
            endTime: '00:24:30',
            text: 'è¡ŒéŠ·åœ˜éšŠçš„å¸‚å ´åˆ†æé¡¯ç¤ºï¼Œæˆ‘å€‘çš„ç›®æ¨™å®¢ç¾¤å°æ–°åŠŸèƒ½éœ€æ±‚å¾ˆé«˜ï¼Œç‰¹åˆ¥æ˜¯ç¤¾äº¤åˆ†äº«å’Œå€‹æ€§åŒ–æ¨è–¦åŠŸèƒ½ã€‚ç«¶å“åˆ†æä¹Ÿå®Œæˆäº†ï¼Œæˆ‘å€‘åœ¨æŠ€è¡“å‰µæ–°æ–¹é¢æœ‰æ˜é¡¯å„ªå‹¢ã€‚å»ºè­°åŠ å¿«ç”¢å“ä¸Šå¸‚æ™‚ç¨‹ï¼Œæ¶ä½”å¸‚å ´å…ˆæ©Ÿã€‚',
            confidence: 0.88
        }
    ];

    // Extend for different sizes
    let sections = [...baseSections];
    let duration = '00:24:30';
    let totalEntries = baseSections.length;

    if (sizeCategory === 'medium') {
        // Duplicate sections with time adjustments for medium size
        const additionalSections = baseSections.map((section, index) => ({
            ...section,
            startTime: addTimeToTimestamp(section.startTime, 25 * 60), // Add 25 minutes
            endTime: addTimeToTimestamp(section.endTime, 25 * 60),
            text: section.text + ' æ¥ä¸‹ä¾†æˆ‘å€‘æ·±å…¥è¨è«–æŠ€è¡“ç´°ç¯€å’Œå¯¦ä½œæ–¹æ¡ˆã€‚' + section.text.substring(0, 100) + '...'
        }));
        sections = [...sections, ...additionalSections];
        duration = '00:49:00';
        totalEntries = sections.length;
    } else if (sizeCategory === 'large') {
        // Create many sections for large transcript
        for (let i = 0; i < 20; i++) {
            const cycleStartMin = (i + 1) * 25;
            const cycleSections = baseSections.map((section, sIndex) => ({
                ...section,
                speaker: `${section.speaker}_${i + 1}`,
                startTime: addTimeToTimestamp(section.startTime, cycleStartMin * 60),
                endTime: addTimeToTimestamp(section.endTime, cycleStartMin * 60),
                text: `[ç¬¬${i + 1}è¼ªè¨è«–] ${section.text} åœ¨é€™å€‹éšæ®µï¼Œæˆ‘å€‘éœ€è¦æ›´æ·±å…¥åœ°åˆ†æå„ç¨®å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆï¼Œè€ƒæ…®æŠ€è¡“å¯è¡Œæ€§ã€æˆæœ¬æ•ˆç›Šã€æ™‚ç¨‹å®‰æ’ç­‰å› ç´ ã€‚` + 'A'.repeat(500 + Math.random() * 1000)
            }));
            sections = [...sections, ...cycleSections];
        }
        duration = '08:45:30';
        totalEntries = sections.length;
    }

    const content = sections.map(section => 
        `[${section.startTime}] ${section.speaker}: ${section.text}`
    ).join('\n');

    return {
        metadata: {
            participants: [...new Set(sections.map(s => s.speaker))],
            duration,
            language: 'zh-tw',
            totalEntries,
            startTime: sections[0].startTime,
            endTime: sections[sections.length - 1].endTime
        },
        content,
        sections
    };
}

function addTimeToTimestamp(timestamp, secondsToAdd) {
    const parts = timestamp.split(':');
    const totalSeconds = parseInt(parts[0]) * 3600 + parseInt(parts[1]) * 60 + parseInt(parts[2]) + secondsToAdd;
    
    const hours = Math.floor(totalSeconds / 3600);
    const minutes = Math.floor((totalSeconds % 3600) / 60);
    const seconds = totalSeconds % 60;
    
    return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
}

// Mock AI function for demonstration
async function mockAIFunction(transcript, options = {}) {
    const delay = Math.random() * 1000 + 500; // Random delay 0.5-1.5s
    await new Promise(resolve => setTimeout(resolve, delay));
    
    const isChunk = options.isChunk;
    const chunkIndex = options.chunkIndex || 0;
    
    if (options.isCombinung) {
        return {
            summary: `## æœƒè­°ç¸½çµ\n\né€™æ˜¯ä¸€å€‹${options.totalSections}å€‹æ®µè½çš„ç¶œåˆæœƒè­°æ‘˜è¦ã€‚æœƒè­°æ¶µè“‹äº†ç”¢å“é–‹ç™¼ã€è¨­è¨ˆã€æ¸¬è©¦å’Œè¡ŒéŠ·ç­‰å¤šå€‹é¢å‘çš„è¨è«–ã€‚\n\n## ä¸»è¦æ±ºç­–\n- åŠ å¿«ç”¢å“é–‹ç™¼é€²åº¦\n- å¼·åŒ–ç”¨æˆ¶é«”é©—è¨­è¨ˆ\n- æå‡æ¸¬è©¦è¦†è“‹ç‡\n\n## å¾ŒçºŒè¡Œå‹•\n- æŠ€è¡“æ¶æ§‹å„ªåŒ–\n- å¸‚å ´æ¨å»£æº–å‚™\n- è·¨éƒ¨é–€å”ä½œå¼·åŒ–`,
            metadata: {
                generatedAt: new Date().toISOString(),
                model: options.model || 'gpt-4.1',
                processingTime: delay,
                usage: { total_tokens: 1500 + Math.floor(Math.random() * 500) }
            }
        };
    }
    
    const summaryContent = isChunk 
        ? `## æœƒè­°æ®µè½ ${chunkIndex + 1} æ‘˜è¦\n\næ­¤æ®µè½è¨è«–äº†${transcript.metadata.participants.join('ã€')}ç­‰åƒèˆ‡è€…çš„å ±å‘Šå…§å®¹ã€‚ä¸»è¦æ¶µè“‹äº†æŠ€è¡“é–‹ç™¼é€²åº¦ã€è¨­è¨ˆå„ªåŒ–æ–¹æ¡ˆä»¥åŠå¸‚å ´ç­–ç•¥èª¿æ•´ç­‰è­°é¡Œã€‚\n\n### é—œéµè¦é»\n- é–‹ç™¼é€²åº¦ç¬¦åˆé æœŸ\n- è¨­è¨ˆæ”¹å–„ç²å¾—æ­£é¢åé¥‹\n- å¸‚å ´æ©Ÿæœƒå€¼å¾—æŠŠæ¡\n\n### å¾ŒçºŒè¿½è¹¤\n- æŠ€è¡“ç´°ç¯€ç¢ºèª\n- æ™‚ç¨‹å®‰æ’èª¿æ•´\n- è³‡æºé…ç½®å„ªåŒ–`
        : `## å®Œæ•´æœƒè­°æ‘˜è¦\n\né€™å ´æœƒè­°è¨è«–äº†Q2ç”¢å“é–‹ç™¼çš„å„å€‹é¢å‘ï¼ŒåŒ…æ‹¬æŠ€è¡“å¯¦ä½œã€è¨­è¨ˆå„ªåŒ–ã€æ¸¬è©¦æµç¨‹å’Œå¸‚å ´ç­–ç•¥ã€‚å„éƒ¨é–€éƒ½æå‡ºäº†å…·é«”çš„é€²åº¦å ±å‘Šå’Œæ”¹å–„å»ºè­°ã€‚\n\n### ä¸»è¦æˆæœ\n- ç”¨æˆ¶èªè­‰ç³»çµ±å®Œæˆ\n- UIè¨­è¨ˆç²å¾—æ­£é¢è©•åƒ¹\n- æ¸¬è©¦è¦†è“‹ç‡é”æ¨™\n- å¸‚å ´åˆ†æå®Œæ•´\n\n### æ±ºç­–äº‹é …\n- åŠ å¿«é–‹ç™¼æ™‚ç¨‹\n- å„ªåŒ–ç”¨æˆ¶é«”é©—\n- å¼·åŒ–å¸‚å ´æ¨å»£\n\n### è¡Œå‹•è¨ˆç•«\n- æŠ€è¡“å„ªåŒ–æŒçºŒé€²è¡Œ\n- è¨­è¨ˆç´°ç¯€æœ€çµ‚ç¢ºèª\n- æ¸¬è©¦æµç¨‹å®Œå–„\n- è¡ŒéŠ·ç­–ç•¥åŸ·è¡Œ`;

    return {
        summary: summaryContent,
        metadata: {
            generatedAt: new Date().toISOString(),
            model: options.model || 'gpt-4.1',
            processingTime: delay,
            usage: { total_tokens: 800 + Math.floor(Math.random() * 400) },
            chunkInfo: isChunk ? {
                chunkIndex,
                totalChunks: options.totalChunks,
                timeRange: transcript.metadata.startTime + ' - ' + transcript.metadata.endTime
            } : undefined
        }
    };
}

// Demonstration scenarios
async function runDemonstrations() {
    console.log('ğŸš€ Teams Transcript Chunking Strategy Demonstration\n');
    console.log('=' * 60);

    const strategy = new ChunkingStrategy();

    // Scenario 1: Small transcript (no chunking needed)
    console.log('\nğŸ“‹ Scenario 1: Small Transcript Analysis');
    console.log('-' * 40);
    
    const smallTranscript = createDemoTranscript('small');
    console.log(`Transcript size: ${smallTranscript.metadata.duration}`);
    console.log(`Participants: ${smallTranscript.metadata.participants.length}`);
    console.log(`Content length: ${smallTranscript.content.length} characters`);
    
    const smallAnalysis = strategy.analyzeChunkingNeeds(smallTranscript, 'openai', 'gpt-4.1');
    console.log(`Token estimate: ${smallAnalysis.tokenCount}`);
    console.log(`Needs chunking: ${smallAnalysis.needsChunking}`);
    console.log(`Complexity: ${smallAnalysis.complexity}`);
    console.log(`Recommended strategy: ${smallAnalysis.recommendedStrategy}`);

    // Scenario 2: Large transcript with different strategies
    console.log('\nğŸ“‹ Scenario 2: Large Transcript with Multiple Strategies');
    console.log('-' * 40);
    
    const largeTranscript = createDemoTranscript('large');
    console.log(`Transcript size: ${largeTranscript.metadata.duration}`);
    console.log(`Participants: ${largeTranscript.metadata.participants.length}`);
    console.log(`Content length: ${largeTranscript.content.length} characters`);
    console.log(`Sections: ${largeTranscript.sections.length}`);
    
    const largeAnalysis = strategy.analyzeChunkingNeeds(largeTranscript, 'openai', 'gpt-4.1');
    console.log(`Token estimate: ${largeAnalysis.tokenCount}`);
    console.log(`Context limit: ${largeAnalysis.contextLimit}`);
    console.log(`Needs chunking: ${largeAnalysis.needsChunking}`);
    console.log(`Estimated chunks: ${largeAnalysis.estimatedChunks}`);
    console.log(`Complexity: ${largeAnalysis.complexity}`);

    // Test different chunking strategies
    console.log('\nğŸ”§ Testing Different Chunking Strategies:');
    
    for (const [strategyName, strategyValue] of Object.entries(CHUNKING_STRATEGIES)) {
        console.log(`\n  ${strategyName}: `, );
        const chunks = strategy.chunkTranscript(largeTranscript, {
            strategy: strategyValue,
            provider: 'openai',
            model: 'gpt-4.1'
        });
        
        console.log(`    Chunks created: ${chunks.length}`);
        console.log(`    Avg chunk size: ${Math.round(chunks.reduce((sum, c) => sum + strategy.estimateTokenCount(c.content), 0) / chunks.length)} tokens`);
        console.log(`    Time ranges: ${chunks[0]?.metadata?.timeRange?.start} â†’ ${chunks[chunks.length-1]?.metadata?.timeRange?.end}`);
    }

    // Scenario 3: Provider comparison
    console.log('\nğŸ“‹ Scenario 3: Provider Comparison');
    console.log('-' * 40);
    
    const providers = [
        { name: 'OpenAI GPT-4.1', provider: 'openai', model: 'gpt-4.1' },
        { name: 'Claude Sonnet 4', provider: 'anthropic', model: 'claude-3-5-sonnet-20241022' }
    ];
    
    providers.forEach(config => {
        const analysis = strategy.analyzeChunkingNeeds(largeTranscript, config.provider, config.model);
        console.log(`\n  ${config.name}:`);
        console.log(`    Context limit: ${analysis.contextLimit.toLocaleString()} tokens`);
        console.log(`    Safe limit: ${analysis.safeLimit.toLocaleString()} tokens`);
        console.log(`    Needs chunking: ${analysis.needsChunking}`);
        console.log(`    Estimated chunks: ${analysis.estimatedChunks}`);
    });

    // Scenario 4: Full processing with progress tracking
    console.log('\nğŸ“‹ Scenario 4: Full Processing with Progress Tracking');
    console.log('-' * 40);
    
    console.log('Processing large transcript with hybrid strategy...');
    
    const progressUpdates = [];
    const progressCallback = (update) => {
        progressUpdates.push(update);
        const percent = Math.round((update.current / update.total) * 100);
        console.log(`  [${percent}%] ${update.stage}: ${update.message}`);
        
        if (update.chunkInfo) {
            console.log(`    â””â”€ Chunk: ${update.chunkInfo.timeRange}, Speakers: ${update.chunkInfo.speakers.join(', ')}, Tokens: ${update.chunkInfo.tokenCount}`);
        }
    };

    try {
        const result = await strategy.processLargeTranscript(
            largeTranscript,
            mockAIFunction,
            {
                provider: 'openai',
                model: 'gpt-4.1',
                strategy: CHUNKING_STRATEGIES.HYBRID,
                language: 'zh-TW'
            },
            progressCallback
        );

        console.log('\nâœ… Processing Complete!');
        console.log(`Processing method: ${result.metadata.processingMethod}`);
        console.log(`Chunks processed: ${result.metadata.chunksProcessed}`);
        console.log(`Chunks failed: ${result.metadata.chunksFailed}`);
        console.log(`Total tokens: ${result.metadata.chunkingSummary.totalTokens.toLocaleString()}`);
        console.log(`Average chunk size: ${result.metadata.chunkingSummary.avgChunkSize} tokens`);
        
        console.log('\nğŸ“„ Final Summary Preview:');
        console.log(result.summary.substring(0, 300) + '...');
        
        console.log('\nğŸ“Š Chunk Details:');
        result.chunkDetails.forEach((detail, index) => {
            console.log(`  Chunk ${index + 1}: ${detail.timeRange.start}-${detail.timeRange.end}, ${detail.tokenCount} tokens, Success: ${detail.success}`);
        });

    } catch (error) {
        console.error('âŒ Processing failed:', error.message);
    }

    // Scenario 5: Performance characteristics
    console.log('\nğŸ“‹ Scenario 5: Performance Characteristics');
    console.log('-' * 40);
    
    console.log('Testing performance with different transcript sizes...');
    
    const sizes = ['small', 'medium', 'large'];
    for (const size of sizes) {
        const transcript = createDemoTranscript(size);
        const startTime = Date.now();
        
        const analysis = strategy.analyzeChunkingNeeds(transcript, 'openai', 'gpt-4.1');
        if (analysis.needsChunking) {
            strategy.chunkTranscript(transcript, {
                provider: 'openai',
                model: 'gpt-4.1',
                strategy: CHUNKING_STRATEGIES.HYBRID
            });
        }
        
        const endTime = Date.now();
        console.log(`  ${size.padEnd(6)}: ${transcript.content.length.toString().padStart(8)} chars, ${(endTime - startTime).toString().padStart(4)}ms`);
    }

    console.log('\nğŸ‰ Demonstration Complete!');
    console.log('\nKey Features Demonstrated:');
    console.log('â€¢ Intelligent chunking need detection');
    console.log('â€¢ Multiple chunking strategies (speaker turns, time-based, semantic, hybrid)');
    console.log('â€¢ Provider-specific optimizations (OpenAI vs Claude)');
    console.log('â€¢ Context overlap for better continuity');
    console.log('â€¢ Progress tracking for large transcripts');
    console.log('â€¢ Error handling and recovery');
    console.log('â€¢ Performance optimization');
    console.log('â€¢ Comprehensive metadata enrichment');
}

// Run the demonstration
if (require.main === module) {
    runDemonstrations().catch(console.error);
}

module.exports = {
    createDemoTranscript,
    mockAIFunction,
    runDemonstrations
};